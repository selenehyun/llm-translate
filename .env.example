# ==============================================================================
# llm-translate Server Configuration
# Copy this file to .env and fill in your values
# ==============================================================================

# ------------------------------------------------------------------------------
# Server Configuration
# ------------------------------------------------------------------------------

# Server port (default: 3000)
TRANSLATE_PORT=3000

# API key for authenticating requests to the translation API
# Generate a secure random key: openssl rand -base64 32
TRANSLATE_API_KEY=your-api-key-here

# ------------------------------------------------------------------------------
# LLM Provider API Keys
# At least one provider key is required for translation to work
# ------------------------------------------------------------------------------

# Anthropic Claude API key (recommended)
# Get your key at: https://console.anthropic.com/
ANTHROPIC_API_KEY=sk-ant-xxxxx

# OpenAI API key
# Get your key at: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-xxxxx

# Ollama server URL (for self-hosted models)
# Default: http://localhost:11434
# Use http://ollama:11434 when running with docker-compose
OLLAMA_BASE_URL=http://localhost:11434
