version: '3.8'

services:
  # ==============================================================================
  # llm-translate API Server
  # ==============================================================================
  llm-translate:
    build: .
    container_name: llm-translate-api
    restart: unless-stopped
    ports:
      - "${TRANSLATE_PORT:-3000}:3000"
    environment:
      - NODE_ENV=production
      - TRANSLATE_API_KEY=${TRANSLATE_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://ollama:11434}
    healthcheck:
      test: ["CMD", "node", "-e", "fetch('http://localhost:3000/health/live').then(r => r.ok ? process.exit(0) : process.exit(1))"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    deploy:
      resources:
        limits:
          memory: 512M
        reservations:
          memory: 256M

  # ==============================================================================
  # Optional: Local Ollama service for self-hosted LLM
  # Enable with: docker compose --profile with-ollama up
  # ==============================================================================
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    profiles:
      - with-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama-models:/root/.ollama
    deploy:
      resources:
        limits:
          memory: 4G

volumes:
  ollama-models:
